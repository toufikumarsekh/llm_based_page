// llm.js
require('dotenv').config();
const OpenAI = require('openai');

const openai = new OpenAI({
    apiKey: process.env.OPENROUTER_API_KEY,
    baseURL: 'https://openrouter.ai/api/v1',
});

/**
 * Generate a response using GPT-4.1 Mini (text-only, multilingual)
 * @param {string} prompt - The user prompt
 * @returns {Promise<string>} - AI-generated response
 */
async function generateLLMResponse(prompt) {
    try {
        console.log('üì§ Prompt sent to LLM:', prompt);

        const response = await openai.chat.completions.create({
            model: 'GPT-3.5-turbo', // ‚úÖ Faster, free, multilingual
            messages: [{ role: 'user', content: prompt }],
        });

        console.log('‚úÖ LLM response:', JSON.stringify(response, null, 2));

        return response.choices[0].message.content.trim();
    } catch (err) {
        console.error('‚ùå LLM API error:', err.response?.data || err.message || err);
        throw err;
    }
}

// ‚úÖ Export for app.js
module.exports = { generateLLMResponse };
